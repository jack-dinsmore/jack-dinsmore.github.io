<html>
<head>
    <meta charset="utf-8">
    <link rel="stylesheet" type = "text/css" href = "https://jack-dinsmore.github.io/blog/html/css/text.css">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async
            src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js">
    </script>
    <script>
window.MathJax = {
    tex: {
        macros: {
            bm: ["\\mathbf {#1}",1],
            parens: ["\\left( #1 \\right)", 1],
            braces: ["\\left\{ #1 \\right\}", 1],
            brackets: ["\\left[ #1 \\right]", 1],
            eval: ["\\left. #1 \\right|", 1],
            fraci: ["{#1} / {#2}", 2],
            expp: ["\\exp\\left( #1 \right)", 1],
            bra: ["\\left\\langle #1 \\right|", 1],
            ket: ["\\left| #1 \\right\\rangle", 1],
            braket: ["\\langle {#1} | {#2} \\rangle", 2],
        }
    }
}
    </script>
    <title>statistics</title>
</head><body><div id="content"><h2><a href="https://jack-dinsmore.github.io/blog/html/index.html">Home</a></h2><h1>Expected Value, Variance, and Covariance</h1>

<p>In the <a href=https://jack-dinsmore.github.io/blog/html/probability.html>previous section</a>, we defined random variables and their basic properties such as their Probability Density Functions (PDFs) and Probability Mass Functions (PMFs). These functions completely describe the random variable in question, and we often want to measure them in science. This is quite difficult to do accurately because a PDF is a function with a separate value for each point in the domain, all of which have to be measured. In such cases, it is often more useful to measure simplifying properties of the PDF instead. These are called the “moments” of the distribution. The \(n^\mathrm{th}\) moment of the PDF \(P(x)\) is defined as
<div class="eq"><div class="eqtext">\[\int dx\, x^n P(x).\]</div><div class="eqnum" id="eq1">(1)</div></div>
for some integer \(n\).
</p>
<p>Below, we will focus on a few important moments and explain why they are useful. We will consider continuous random variables which have PDFs, though many of the results also apply to discrete random variables.
</p>
<button class="collapsible"><h2>First moment: Expected value</h2></button><div class="section">

<p>The <i>expected value</i> of a random variable \(X\) is defined as its first-order moment:
<div class="eq"><div class="eqtext">\[\langle X\rangle = \int dx\, x P(x).\]</div><div class="eqnum" id="eq2">(2)</div></div>
It is effectively the average of all values \(X\) could have, weighted by their probability.
</p>
<p>The introduction of this post implied that moments are easier to measure than the PDF directly. To see this, let’s design an experiment that attempts to measure \(\langle X \rangle\). We’ll make \(n\) independent measurements of \(X\), which we call \(X_1, X_2,\dots X_n\). A good <i>estimator</i> for \(\langle X\rangle\) could be to take the average of these measurements:
<div class="eq"><div class="eqtext">\[M = \frac{1}{n}\sum_{i=1}^n X_i.\]</div><div class="eqnum" id="eq3">(3)</div></div>
</p>
<p>The PDF of \(M\) is a multivariate PDF, but because \(X_i\) are all independent, it simplifies to \(P(x_1,\dots,x_n) = P(x_1)\cdots P(x_n)\). Thus, the expected value of \(M\) is
<div class="eq"><div class="eqtext">\[\langle M \rangle = \int dx_1\cdots dx_n\, \brackets{\frac{1}{n} \sum_{i=1}^n x_i} P(x_1,\dots,x_n)\]</div><div class="eqnum" id="eq4">(4)</div></div>
<div class="eq"><div class="eqtext">\[ = \frac{1}{n} \sum_{i=1}^n \int dx_i\, x_i P(x_i)\]</div><div class="eqnum" id="eq5">(5)</div></div>
<div class="eq"><div class="eqtext">\[ = \frac{1}{n} \sum_{i=1}^n \langle X_i \rangle\]</div><div class="eqnum" id="eq6">(6)</div></div>
<div class="eq"><div class="eqtext">\[ = \langle X \rangle.\]</div><div class="eqnum" id="eq7">(7)</div></div>
</p>
<p>We have shown that the expected value of this average is the expected value of the random variable itself, so that our estimator \(M\) is an accurate measurement of \(\langle X \rangle\). But to show that it’s precise, we must introduce the concept of variance.
</p>
</div> <button class="collapsible"><h2>Second moment: Variance</h2></button><div class="section">

<p>The <i>variance</i> of \(X\) is defined as
<div class="eq"><div class="eqtext">\[\mathrm{Var}\ X = \int dx\, (x - \langle X \rangle)^2 P(x) = \langle (X - \langle X \rangle)^2 \rangle.\]</div><div class="eqnum" id="eq8">(8)</div></div>
This is a second-order moment of \(X\). Its value is positive, so we can also define the <i>standard deviation</i> \(\sigma_X\) to be such that \(\sigma_X^2 = \mathrm{Var}\ X\). Manipulating the above integral shows that an alternative form of variance is
</p>

<div class="eq"><div class="eqtext">\[\mathrm{Var}\ X = \langle X^2 \rangle - \langle X \rangle^2\]</div><div class="eqnum" id="eq9">(9)</div></div>

<p>which is often useful. 
</p>
<p>Variance is an extremely important concept because it characterizes the width of a probability distribution. A PDF with variance of zero must be a delta function &mdash; zero probability everywhere except at one value which is the mean. On the other hand, a PDF with high variance could take on a great many widely separated values.
</p>
<p>Returning to the experiment we introduced in the previous section, \(M\) is only a precise estimator for \(\langle X \rangle\) if it has low variance. Let’s check this by computing \(\langle M^2 \rangle\) and applying the second form of variance above.
<div class="eq"><div class="eqtext">\[\langle M^2 \rangle = \int dx_1\cdots dx_n\, \brackets{\frac{1}{n} \sum_{i=1}^n x_i}^2 P(x_1)\cdots P(x_n)\]</div><div class="eqnum" id="eq10">(10)</div></div>
<div class="eq"><div class="eqtext">\[= \frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n \int dx_i dx_j\, x_i x_j P(x_i)P(x_j)\]</div><div class="eqnum" id="eq11">(11)</div></div>
<div class="eq"><div class="eqtext">\[= \frac{1}{n^2} \sum_{i=1}^n \sum_{j\neq i} \langle X \rangle^2 + \frac{1}{n^2} \sum_{i=1}^n \langle X^2 \rangle\]</div><div class="eqnum" id="eq12">(12)</div></div>
<div class="eq"><div class="eqtext">\[= \frac{n-1}{n}\langle X \rangle^2 + \frac{1}{n} \langle X^2 \rangle\]</div><div class="eqnum" id="eq13">(13)</div></div>
<div class="eq"><div class="eqtext">\[= \langle X \rangle^2 + \frac{1}{n} \mathrm{Var}\ X\]</div><div class="eqnum" id="eq14">(14)</div></div>
where the second line expanded the square, the third line separated the components where \(i\neq j\) and \(i= j\), the fourth counted the number of terms in the sum, and the fifth substituted in <a href=https://jack-dinsmore.github.io/blog/html/variance.html#eq9>Eq. 9</a>.
</p>
<p>Finally, the variance of \(M\) must be
<div class="eq"><div class="eqtext">\[\mathrm{Var}\ M = \frac{1}{n} \mathrm{Var}\ X\]</div><div class="eqnum" id="eq15">(15)</div></div>
or equivalently
<div class="eq"><div class="eqtext">\[\sigma_M = \frac{\sigma_X}{\sqrt{n}}.\]</div><div class="eqnum" id="eq16">(16)</div></div>
This shows that \(M\) is a good estimator for the expected value of \(X\), because as \(n\) increases \(M\) has low variance, becoming more and more precise.
</p>
<p>This experiment illustrated a common process in statistics. Whenever one wants to measure a property of a random variable, one creates an estimator, shows that it is <i>unbiased</i> (i.e. its mean is the thing attempted to be measured) and that it is <i>minimum-variance</i> (i.e. its variance is as low as possible in the limit of large data).
</p>
<p>An exercise for the reader is to show that the minimum variance estimator of the variance of a distribution is 
<div class="eq"><div class="eqtext">\[V = \frac{1}{n - 1} \sum_{i=1}^n (X_i - M)^2.\]</div><div class="eqnum" id="eq17">(17)</div></div>
and, like \(M\), exhibits \(1/\sqrt{n}\) standard deviation.
</p>

</div> <button class="collapsible"><h2>Covariance</h2></button><div class="section">

<p>For multivariate probability distributions, it’s possible to define a different type of second moment, called <i>covariance</i>
<div class="eq"><div class="eqtext">\[\mathrm{Cov} [X, Y] = \big\langle (X - \langle X \rangle)(Y- \langle Y\rangle)\big\rangle.\]</div><div class="eqnum" id="eq18">(18)</div></div>
For a set of \(n\) random variables \(X_1,\dots,X_n\) one can define a <i>covariance matrix</i> \(\Sigma\) whose entries are
<div class="eq"><div class="eqtext">\[\Sigma_{ij} = \mathrm{Cov} [X_i, X_j].\]</div><div class="eqnum" id="eq19">(19)</div></div>
Notice that  \(\mathrm{Cov} [X, Y] = \mathrm{Cov} [Y, X]\) and \(\mathrm{Cov} [X, X] = \mathrm{Var}\ X\) by definition. Equivalently, we could say that the covariance matrix \(\Sigma\) is symmetric, and its diagonal contains the variances of \(X_i\).
</p>
<p>An interesting fact about covariance is that if \(X\) and \(Y\) are independent, then \(\mathrm{Cov} [X, Y] = 0\). You can prove this from the first equation of this section. Just as variance encapsulated the width of a probability distribution, covariance therefore encapsulates how dependent two variables are on each other. If the covariance matrix contains off-diagonal elements, its random variables are dependent.
</p>
<p>The covariance matrix is a very useful mathematical object, which we will unpack later.
</p>
</div> <button class="collapsible"><h2>Higher order moments</h2></button><div class="section">
<p>One can continue to form third and fourth order moments, which can likewise be estimated from data. These are generally called <i>skewness</i> (third order) and <i>kurtosis</i> (fourth order), and qualitatively they measure asymmetry in the PDF and the fraction of probability contained in the tails of the distribution. Their estimators possess approximately \(1/\sqrt{n}\) standard deviation just as the mean estimator did. We will not discuss these much.
</p>
</div> <h1>Consequences </h1>


<button class="collapsible"><h2>Central limit theorem</h2></button><div class="section">

<p>In the above sections, we showed that the average \(M\) of many independent random variables \(X_1,\dots,X_n\) drawn from the same distribution has mean \(\langle X \rangle\) and standard deviation \(\sigma_X / \sqrt{n}\). But we stopped short of computing the full PDF \(P_M(x)\).
</p>
<p>You might expect that \(P_M(m)\) depends on the details of \(P_X(x)\). But for large \(n\), the <i>central limit theorem</i> tells us this is not the case. This theorem is arguably one of the most important and most used theorems in statistics.
</p>
<p>The central limit theorem (CLT) states that, under some light constraints, \(M\) is Gaussian-distributed in the limit of large \(n\) regardless of \(P_X(x)\). That is,

<div class="eq"><div class="eqtext">\[P_M(m) \rightarrow \sqrt{\frac{n}{2\pi \sigma_X^2}}\exp\parens{-\frac{(m-\langle X \rangle)^2}{2 \sigma_X^2/n}}\]</div><div class="eqnum" id="eq20">(20)</div></div>
The applications of the CLT are practically boundless. If you don’t know the PDF of \(X\), which is a common problem, an average over enough data points drawn from \(X\) will be Gaussian-distributed. A histogram, for example, will have Gaussian-distributed error bars on each bin. Likewise a function of the average will have easily predictable error bars, insensitive to potential tails in the distribution of \(P_X(x)\).
</p>
<p>What follows is one of many possible proofs, based on Fourier series. For the sake of simplicity, we work with the quantity \(S = nM\), whose PDF is

<div class="eq"><div class="eqtext">\[P_S(s) = \int dx_1\cdots dx_n\, P_X(x_1)\cdots P_X(x_n) \delta\parens{s - \sum_{i=1}^n x_1}.\]</div><div class="eqnum" id="eq21">(21)</div></div>
</p>
<p>This is the convolution of \(P_X\) with itself \(n\) times. The “convolution theorem” dictates that the Fourier transform of the real-space product of functions \(f(x)g(x)\) is equal to the convolution of their Fourier transforms, \(\widetilde f(k)\) and \(\widetilde g(k)\). The inverse of this statement demonstrates that the convolution in <a href=https://jack-dinsmore.github.io/blog/html/variance.html#eq21>Eq. 21</a> satisfies

<div class="eq"><div class="eqtext">\[\widetilde P_S(k) = \widetilde P_X(k)^n\]</div><div class="eqnum" id="eq22">(22)</div></div>
or
<div class="eq"><div class="eqtext">\[\ln \widetilde P_S(k) = n \ln \widetilde P_X(k).\]</div><div class="eqnum" id="eq23">(23)</div></div>
We are interested in the behavior of \(\ln \widetilde P_S(k)\) near its maximum, which contributes most to \(P_S(s)\). Due to the large-\(n\) multiplication on the right hand side, \(\widetilde P_S(k)\) is exponentially suppressed except when \(k\) is very close to the maximizing value \(k_0\). We may therefore approximate \(\ln \widetilde P_X(k)\) by its lowest order Taylor series near its maximum
<div class="eq"><div class="eqtext">\[\ln \widetilde P_S(k) = na_1 - \frac{n}{2}(k-k_0)^2 a_2\]</div><div class="eqnum" id="eq24">(24)</div></div>
where \(a_1\) and \(a_2>0\) are Taylor series coefficients. Removing the logarithm, \(P_S(k)\) is a Gaussian
<div class="eq"><div class="eqtext">\[\widetilde P_S(k) \propto \exp \parens{-a_2\frac{n(k-k_0)^2}{2}}\]</div><div class="eqnum" id="eq25">(25)</div></div>
and the Fourier transform of a Gaussian is also a Gaussian
<div class="eq"><div class="eqtext">\[P_S(x) \propto \exp \parens{-\frac{(x-\mu)^2}{2\sigma’^2}} \implies P_M(x) \propto \exp \parens{-\frac{(x-\mu)^2}{2\sigma^2}}\]</div><div class="eqnum" id="eq26">(26)</div></div>
for some parameters \(\sigma\) and \(\mu\). We showed in the sections on expected value and variance that \(P_S(x)\) has expected value \(\mu = \langle X \rangle\) and standard deviation \(\sigma = \sigma_X/\sqrt{n}\). Asserting these findings reproduces the central limit theorem <a href=https://jack-dinsmore.github.io/blog/html/variance.html#eq20>Eq. 20</a>.
</p>

</div> <button class="collapsible"><h2>Propagation of uncertainty</h2></button><div class="section">

<p>The last property we wish to note about variances is how to “propagate” variances through functions. Specifically, if we know \(\sigma_X\) and if \(Y = f(X)\), then what is \(\sigma_Y\)?
</p>
<p>While there is a general solution, a common approximation is made. Suppose that \(P_X(x)\) is narrowly peaked around its mean, so that \(f(x)\) is slowly varying over the region where \(P_X(x)\) is large. Then we may approximate \(f(x)\) as linear in that region:
<div class="eq"><div class="eqtext">\[Y = f’(\langle X \rangle)(X - \langle X \rangle) + f(\langle X \rangle).\]</div><div class="eqnum" id="eq27">(27)</div></div>
The definition of variance now simply states
<div class="eq"><div class="eqtext">\[\sigma_Y = |f’(\langle X \rangle)|\sigma_X.\]</div><div class="eqnum" id="eq28">(28)</div></div>
</p>
<p>The <a href=https://jack-dinsmore.github.io/blog/html/bayesian.html>next section</a> represents our first application of probability theory to statistics; we will use the tools we’ve learned to create a theory fitting models to data.
</div></div><div id="footer">
Copyright &copy; 2024 Jack Dinsmore. &emsp;Updated September 23. &emsp;Version 0.1</div></body>
<script>
var coll = document.getElementsByClassName("collapsible");
var i;

for (i = 0; i < coll.length; i++) {{
    coll[i].addEventListener("click", function() {{
        this.classList.toggle("active");
        var content = this.nextElementSibling;
        if (content.style.maxHeight === "0px"){{
            content.style.maxHeight = content.scrollHeight+"px";
        }} else {{
            content.style.maxHeight = "0px";
        }}
    }});
}}
</script></html>
